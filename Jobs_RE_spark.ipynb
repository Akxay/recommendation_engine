{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Building a large scale job Recommendation Engine using Implicit data in pyspark **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.ml.recommendation import ALS\n",
    "# from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conf = SparkConf().setAppName(\"jobRecommendationEngine\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size is  100004\n",
      "\n",
      "Columns are: userId,jobId,Clicks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 31, 3), (1, 1029, 66), (1, 1061, 93)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load job Clicks file into rdd\n",
    "datasets_path=os.getcwd() + \"/RE_data\"\n",
    "ratings_file = os.path.join(datasets_path, 'job_clicks.csv')\n",
    "ratings_raw_data = sc.textFile(\"file:///\" + ratings_file)\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "ratings_data = ratings_raw_data.filter(lambda line: line != ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),int(float(tokens[2])))).cache()\n",
    "\n",
    "print (\"train size is \", ratings_data.count())\n",
    "print('\\nColumns are:', ratings_raw_data_header)\n",
    "ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns are: jobID,job_category\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'IT'), (2, 'IT'), (3, 'IT')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load jobs category file into rdd\n",
    "jobs_file = os.path.join(datasets_path, 'jobs.csv')\n",
    "jobs_raw_data = sc.textFile(\"file:///\" + jobs_file)\n",
    "jobs_raw_data_header = jobs_raw_data.take(1)[0]\n",
    "\n",
    "jobs_data = jobs_raw_data.filter(lambda line: line!=jobs_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1])).cache()\n",
    "print('\\nColumns are:', jobs_raw_data_header)\n",
    "jobs_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 59902, validation: 20112, test: 19990\n"
     ]
    }
   ],
   "source": [
    "rddTraining, rddValidating, rddTesting = ratings_data.randomSplit([6,2,2], seed=1001)\n",
    "\n",
    "#Add user ratings in the training model\n",
    "nbValidating = rddValidating.count()\n",
    "nbTesting    = rddTesting.count()\n",
    "\n",
    "print(\"Training: %d, validation: %d, test: %d\" % (rddTraining.count(), nbValidating, rddTesting.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am using RMSE but for these kind of problems where we have implicit\n",
    "features, it is better to use ** Mean Percentage Ranking (MPR) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[START how_far]\n",
    "def howFarAreWe(model, against, sizeAgainst):\n",
    "    againstNoRatings = against.map(lambda x: (int(x[0]), int(x[1])) )\n",
    "    againstWiRatings = against.map(lambda x: ((int(x[0]),int(x[1])), int(x[2])) )\n",
    "    predictions = model.predictAll(againstNoRatings).map(lambda p: ( (p[0],p[1]), p[2]) )\n",
    "    predictionsAndRatings = predictions.join(againstWiRatings).values()    \n",
    "    return sqrt(predictionsAndRatings.map(lambda s: (s[0] - s[1]) ** 2).reduce(add) / float(sizeAgainst))\n",
    "#[END how_far]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 10 0.1\n",
      "Best so far:55.682865\n",
      "5 5 20 0.1\n",
      "Best so far:55.664132\n",
      "5 5 40 0.1\n",
      "Best so far:55.656700\n",
      "10 5 20 0.1\n",
      "Best so far:55.646255\n",
      "10 5 40 0.1\n",
      "Best so far:55.643666\n",
      "20 5 20 0.1\n",
      "Best so far:55.642589\n",
      "10 5 20 1\n",
      "Best so far:55.638122\n",
      "10 5 40 1\n",
      "Best so far:55.638103\n",
      "20 5 20 1\n",
      "Best so far:55.637189\n",
      "20 5 40 1\n",
      "Best so far:55.627915\n",
      "10 5 40 10\n",
      "Best so far:55.622670\n",
      "Rank 5\n",
      "Regul 10.000000\n",
      "Iter 10\n",
      "Dist 55.622670\n",
      "Alpha 40.000000\n"
     ]
    }
   ],
   "source": [
    "#finding best set of parameters\n",
    "ranks  = [5,10,15,20]\n",
    "reguls = [0.1, 1,10]\n",
    "iters  = [5,10,20]\n",
    "alpha = [10, 20, 40]\n",
    "\n",
    "finalModel = None\n",
    "finalRank  = 0\n",
    "finalRegul = float(0)\n",
    "finalIter  = -1\n",
    "finalDist   = float(300)\n",
    "finalAlpha = float(0)\n",
    "\n",
    "#[START train_model]\n",
    "for cRank, cRegul, cIter, cAlpha in itertools.product(ranks, reguls, iters, alpha):\n",
    "    model = ALS.trainImplicit(rddTraining, cRank, cIter, float(cRegul),alpha=float(cAlpha))\n",
    "    dist = howFarAreWe(model, rddValidating, nbValidating)\n",
    "    if dist < finalDist:\n",
    "        print(cIter, cRank,cAlpha,cRegul)\n",
    "        print(\"Best so far:%f\" % dist)\n",
    "        finalModel = model\n",
    "        finalRank  = cRank\n",
    "        finalRegul = cRegul\n",
    "        finalIter  = cIter\n",
    "        finalDist  = dist\n",
    "        finalAlpha  = cAlpha \n",
    "#[END train_model]\n",
    "\n",
    "print(\"Rank %i\" % finalRank) \n",
    "print(\"Regul %f\" % finalRegul) \n",
    "print(\"Iter %i\" % finalIter)  \n",
    "print(\"Dist %f\" % finalDist) \n",
    "print(\"Alpha %f\" % finalAlpha) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 57.19030098939986\n"
     ]
    }
   ],
   "source": [
    "model = ALS.trainImplicit(rddTraining, rank=finalRank, iterations=finalIter, lambda_= float(finalRegul),alpha=float(finalAlpha))\n",
    "# Calculate all predictions\n",
    "rddTesting_withoutclicks = rddTesting.map(lambda r: ((r[0], r[1])))\n",
    "predictions = model.predictAll(rddTesting_withoutclicks).map(lambda r: ((r[0], r[1]), (r[2])))\n",
    "predictions.take(3)\n",
    "# user id, node_id, actual clickss,pred clickss -> df below\n",
    "rates_and_preds = rddTesting.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions) \n",
    "rates_and_preds.take(3)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is %s' % (error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------------------+\n",
      "| _1|  _2|  _3|                _4|\n",
      "+---+----+----+------------------+\n",
      "|309|3671|32.0|1.0062152515438365|\n",
      "| 42|2916|68.0|0.9550179359043851|\n",
      "|547|3741|27.0|0.9653563646571577|\n",
      "|243|2617|11.0|1.0072970809312765|\n",
      "+---+----+----+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "x = rates_and_preds.map(lambda x : (x[0][0],x[0][1],x[1][0],x[1][1]))\n",
    "hasattr(x, \"toDF\")\n",
    "x.toDF().show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total clicks and average clicks given to each job by different users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_and_averages(ID_and_ratings_tuple):    \n",
    "    nratings = len(ID_and_ratings_tuple[1]) \n",
    "    return ID_and_ratings_tuple[0], (nratings, sum([float(val) for val in ID_and_ratings_tuple[1]])/nratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 107), (97328, 1), (4, 13)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_ID_with_ratings_RDD = (ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "job_ID_with_ratings_RDD_updated = job_ID_with_ratings_RDD.map(lambda x : (x[0], list(x[1])))\n",
    "job_ID_with_avg_ratings_RDD = job_ID_with_ratings_RDD_updated.map(get_counts_and_averages)  # count and average rating\n",
    "job_rating_counts_RDD = job_ID_with_avg_ratings_RDD.map(lambda x: (int(x[0]), x[1][0]))    # rating count per job\n",
    "job_rating_counts_RDD.cache()\n",
    "job_rating_counts_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(161944, (312, 0.2807878695601902)), (161944, (296, 0.10213990447938404)), (161944, (96, -0.1034895934494533)), (161944, (440, -0.15559400922318423)), (161944, (536, 0.0585588119071474)), (161944, (600, 0.05502822506654903)), (161944, (16, -0.19563889120453504)), (161944, (320, -0.0563454939003778)), (161944, (216, -0.030783395765506455)), (161944, (408, 0.20569715889025542))]\n"
     ]
    }
   ],
   "source": [
    "# get user-wise jobs clicked\n",
    "all_users_ratings_RDD = ratings_data.map(lambda x: (x[0], x[1])).groupByKey()\n",
    "all_users_ratings_RDD = all_users_ratings_RDD.map(lambda x : (x[0], list(x[1])))    # jobs clicked by each user\n",
    "\n",
    "### finding unrated jobs by each user- we will use this set for model's prediction/recommendations\n",
    "job_ids = set(jobs_data.map(lambda x : x[0]).toLocalIterator()) # list of all job ids\n",
    "unrated_jobs_RDD = all_users_ratings_RDD.map(lambda x: (x[0], list((job_ids) - set(x[1]))))\n",
    "\n",
    "# #create user_id and unrated job id pairs\n",
    "unrated_userjobs_RDD = unrated_jobs_RDD.flatMap(lambda x : [(x[0],i) for i in x[1]])\n",
    "\n",
    "# # #model predictions for each user and not clicked job pairs\n",
    "recommendations_RDD = model.predictAll(unrated_userjobs_RDD)\n",
    "recommended_jobs_rating_RDD = recommendations_RDD.map(lambda x: (x.product,(x.user, x.rating)))\n",
    "recommended_jobs_rating_RDD.cache()\n",
    "print (recommended_jobs_rating_RDD.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining job title and total number of clicks received by each job for further filtering recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(312, (27648, 'Business Intelligence', 0.23, 1)),\n",
       " (296, (27648, 'Business Intelligence', -0.19, 1)),\n",
       " (96, (27648, 'Business Intelligence', 0.37, 1))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #     # converting id into int for job_clicks_count RDD to perform join\n",
    "# job_clicks_counts_RDD_updated = job_clicks_counts_RDD.map(lambda x: (int(x[0]), x[1]))\n",
    "\n",
    "# join job name with job id, predicted rating for job and total number of ratings received by each job\n",
    "recommendations_rating_title_and_count_RDD = recommended_jobs_rating_RDD.join(jobs_data).join(job_rating_counts_RDD)\n",
    "recommendations_rating_title_and_count_RDD = recommendations_rating_title_and_count_RDD.map(lambda r: (r[0], r[1][0][1], r[1][0][0][0],round(r[1][0][0][1],2),r[1][1]))\n",
    "recommendations_rating_title_and_count_RDD = recommendations_rating_title_and_count_RDD.map(lambda x: (x[2],(x[0],x[1], x[3],x[4])))\n",
    "recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Top 5 recommendations **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+---------------------+--------------------+----------+\n",
      "|user_id|job_recommendations|        job_category|preference_confidence|            rec_date|rec_number|\n",
      "+-------+-------------------+--------------------+---------------------+--------------------+----------+\n",
      "|     26|              41569|                  IT|                 1.13|2019-02-17 00:58:...|         1|\n",
      "|     26|             116797|           Marketing|                 1.07|2019-02-17 00:58:...|         2|\n",
      "|     26|             106489|              Retial|                 1.07|2019-02-17 00:58:...|         3|\n",
      "|     26|              76093|Business Intellig...|                 1.06|2019-02-17 00:58:...|         4|\n",
      "|     26|               7022|                  HR|                 1.06|2019-02-17 00:58:...|         5|\n",
      "|     29|                588|                  HR|                 1.07|2019-02-17 00:58:...|         1|\n",
      "|     29|                349|                  IT|                 1.07|2019-02-17 00:58:...|         2|\n",
      "|     29|                380|                  IT|                 1.07|2019-02-17 00:58:...|         3|\n",
      "|     29|                236|             Banking|                 1.07|2019-02-17 00:58:...|         4|\n",
      "|     29|                153|                  IT|                 1.07|2019-02-17 00:58:...|         5|\n",
      "|    474|                308|                  HR|                 1.05|2019-02-17 00:58:...|         1|\n",
      "|    474|                707|                  HR|                 1.05|2019-02-17 00:58:...|         2|\n",
      "|    474|                471|             Finance|                 1.04|2019-02-17 00:58:...|         3|\n",
      "|    474|                 18|                  IT|                 1.04|2019-02-17 00:58:...|         4|\n",
      "|    474|                 24|                  IT|                 1.03|2019-02-17 00:58:...|         5|\n",
      "+-------+-------------------+--------------------+---------------------+--------------------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter only those jobs which have been clicked by atleast 20 users\n",
    "# take only top5 jobs by sorting based on preference confidence\n",
    "top_jobs = recommendations_rating_title_and_count_RDD.groupBy(lambda x : x[0])\\\n",
    "                               .map(lambda x : list(x[1]))\\\n",
    "                               .map(lambda r: [i for i in r if i[1][3] > 20])\\\n",
    "                               .map(lambda a: [i for i in sorted(a, key=lambda x: -x[1][2])[:5]])   \n",
    "\n",
    "#preparing dataframe to insert in Database\n",
    "rec_jobs_df = top_jobs.map(lambda x: [(i[0],i[1][0],i[1][1],i[1][2]) for i in x]).flatMap(lambda x: x).toDF()\\\n",
    "                                .withColumnRenamed(\"_1\", \"user_id\")\\\n",
    "                                .withColumnRenamed(\"_2\", 'job_recommendations')\\\n",
    "                                .withColumnRenamed(\"_3\", 'job_category')\\\n",
    "                                .withColumnRenamed(\"_4\", 'preference_confidence')\\\n",
    "                                .withColumnRenamed(\"_5\", \"total_clicks\")\n",
    "                \n",
    "# #final recommendation engine dataframe to be saved in Database\n",
    "final_df_rec_eng = rec_jobs_df.withColumn(\"rec_date\", sf.lit(datetime.datetime.now()).cast(TimestampType()))   \n",
    "final_df_rec_eng = final_df_rec_eng.withColumn(\"rec_number\", sf.row_number().over(Window.partitionBy(\"user_id\").orderBy(desc(\"preference_confidence\"))))    \n",
    "final_df_rec_eng.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before showing the top 5 recommendations, we can filter them based on job category to show only \n",
    "recommendations from the categories users have previously looked at.\n",
    "Also, Here, I am using RMSE but for these kind of problems where we have implicit\n",
    "features, it is better to use Mean Percentage Ranking (MPR) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "\n",
    "# model_path = os.path.join('..', 'models', 'job_lens_als')\n",
    "\n",
    "# # Save and load model\n",
    "# model.save(sc, model_path)\n",
    "# same_model = MatrixFactorizationModel.load(sc, model_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
